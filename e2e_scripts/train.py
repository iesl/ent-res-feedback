import json
import os
import time
from collections import defaultdict
from typing import Dict
from typing import Tuple
import math
import logging
import random
import copy
import pickle

import torch
import wandb
from torch.utils.data import DataLoader
import numpy as np
from sklearn.metrics.cluster import v_measure_score
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import precision_recall_fscore_support
from tqdm import tqdm

from e2e_pipeline.model import EntResModel
from e2e_pipeline.pairwise_model import PairwiseModel
from s2and.consts import PREPROCESSED_DATA_DIR
from s2and.data import S2BlocksDataset
from s2and.eval import b3_precision_recall_fscore
from utils.parser import Parser

from IPython import embed


logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s', datefmt='%m/%d/%Y %H:%M:%S',
                    level=logging.INFO)
logger = logging.getLogger(__name__)

# Default hyperparameters
DEFAULT_HYPERPARAMS = {
    # Dataset
    "dataset": "pubmed",
    "dataset_random_seed": 1,
    "subsample_sz": -1,
    "subsample_dev": True,
    # Run config
    "run_random_seed": 17,
    # Data config
    "convert_nan": False,
    "nan_value": -1,
    "drop_feat_nan_pct": -1,
    "normalize_data": True,
    # Model config
    "neumiss_deq": False,
    "neumiss_depth": 20,
    "hidden_dim": 512,
    "n_hidden_layers": 2,
    "dropout_p": 0.1,
    "dropout_only_once": True,
    "batchnorm": True,
    "hidden_config": None,
    "activation": "leaky_relu",
    "negative_slope": 0.01,
    # Solver config
    "sdp_max_iters": 50000,
    "sdp_eps": 1e-3,
    # Training config
    "batch_size": 1,
    "lr": 1e-4,
    "n_epochs": 5,
    "weighted_loss": True,  # Only applies to pairwise model currently; TODO: Implement for e2e
    "use_lr_scheduler": True,
    "lr_scheduler": "plateau",  # "step"
    "lr_factor": 0.7,
    "lr_min": 1e-6,
    "lr_scheduler_patience": 10,
    "lr_step_size": 200,
    "lr_gamma": 0.1,
    "weight_decay": 0.01,
    "dev_opt_metric": 'b3_f1',  # e2e: {'vmeasure', 'b3_f1'}; pairwise: {'auroc', 'f1'}
    "overfit_batch_idx": -1
}

def read_blockwise_features(pkl):
    blockwise_data: Dict[str, Tuple[np.ndarray, np.ndarray]]
    with open(pkl,"rb") as _pkl_file:
        blockwise_data = pickle.load(_pkl_file)
    return blockwise_data


def get_dataloaders(dataset, dataset_seed, convert_nan, nan_value, normalize, subsample_sz, subsample_dev,
                    pairwise_mode, batch_size):
    train_pkl = f"{PREPROCESSED_DATA_DIR}/{dataset}/seed{dataset_seed}/train_features.pkl"
    val_pkl = f"{PREPROCESSED_DATA_DIR}/{dataset}/seed{dataset_seed}/val_features.pkl"
    test_pkl = f"{PREPROCESSED_DATA_DIR}/{dataset}/seed{dataset_seed}/test_features.pkl"

    train_dataset = S2BlocksDataset(read_blockwise_features(train_pkl), convert_nan=convert_nan, nan_value=nan_value,
                                    scale=normalize, subsample_sz=subsample_sz, pairwise_mode=pairwise_mode)
    train_dataloader = DataLoader(train_dataset, shuffle=False, batch_size=batch_size)

    val_dataset = S2BlocksDataset(read_blockwise_features(val_pkl), convert_nan=convert_nan, nan_value=nan_value,
                                  scale=normalize, scaler=train_dataset.scaler,
                                  subsample_sz=subsample_sz if subsample_dev else -1, pairwise_mode=pairwise_mode)
    val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)

    test_dataset = S2BlocksDataset(read_blockwise_features(test_pkl), convert_nan=convert_nan, nan_value=nan_value,
                                   scale=normalize, scaler=train_dataset.scaler, pairwise_mode=pairwise_mode)
    test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)

    return train_dataloader, val_dataloader, test_dataloader

def uncompress_target_tensor(compressed_targets, make_symmetric=True):
    n = round(math.sqrt(2 * compressed_targets.size(dim=0))) + 1
    # Convert the 1D pairwise-similarities list to nxn upper triangular matrix
    ind0, ind1 = torch.triu_indices(n, n, offset=1)
    target = torch.eye(n, device=device)
    target[ind0, ind1] = compressed_targets
    if make_symmetric:
        target[ind1, ind0] = compressed_targets
    return target

# Count parameters in the model
def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad)


def get_matrix_size_from_triu(triu):
    return round(math.sqrt(2 * len(triu))) + 1


def compute_b3_f1(true_cluster_ids, pred_cluster_ids):
    """
    Compute the B^3 variant of precision, recall and F-score.
    Returns:
        Precision
        Recall
        F1
        Per signature metrics
        Overmerging ratios
        Undermerging ratios
    """
    true_cluster_dict, pred_cluster_dict = defaultdict(list), defaultdict(list)
    for i in range(len(true_cluster_ids)):
        true_cluster_dict[true_cluster_ids[i]].append(i)
        pred_cluster_dict[pred_cluster_ids[i].item()].append(i)
    return b3_precision_recall_fscore(true_cluster_dict, pred_cluster_dict)


def evaluate(model, dataloader, overfit_batch_idx=-1):
    n_features = dataloader.dataset[0][0].shape[1]
    vmeasure, b3_f1, sigs_per_block = [], [], []
    for (idx, batch) in enumerate(tqdm(dataloader, desc='Evaluating')):
        if overfit_batch_idx > -1:
            if idx < overfit_batch_idx:
                continue
            if idx > overfit_batch_idx:
                break
        data, target, cluster_ids = batch
        data = data.reshape(-1, n_features).float()
        if data.shape[0] == 0:
            # Only one signature in block -> predict correctly
            vmeasure.append(1.)
            b3_f1.append(1.)
            sigs_per_block.append(1)
        else:
            block_size = get_matrix_size_from_triu(data)
            cluster_ids = np.reshape(cluster_ids, (block_size, ))
            target = target.flatten().float()
            sigs_per_block.append(block_size)

            # Forward pass through the e2e model
            data, target = data.to(device), target.to(device)
            _ = model(data, block_size)
            predicted_cluster_ids = model.hac_cut_layer.cluster_labels.detach()

            # Compute clustering metrics
            vmeasure.append(v_measure_score(predicted_cluster_ids, cluster_ids))
            b3_f1_metrics = compute_b3_f1(cluster_ids, predicted_cluster_ids)
            b3_f1.append(b3_f1_metrics[2])

    vmeasure = np.array(vmeasure)
    b3_f1 = np.array(b3_f1)
    sigs_per_block = np.array(sigs_per_block)

    return np.sum(vmeasure * sigs_per_block) / np.sum(sigs_per_block), \
           np.sum(b3_f1 * sigs_per_block) / np.sum(sigs_per_block)


def evaluate_pairwise(model, dataloader, overfit_batch_idx=-1, mode="macro", return_pred_only=False,
                      thresh_for_f1=0.5):
    n_features = dataloader.dataset[0][0].shape[1]
    y_pred, targets = [], []
    for (idx, batch) in enumerate(tqdm(dataloader, desc='Evaluating')):
        if overfit_batch_idx > -1:
            if idx < overfit_batch_idx:
                continue
            if idx > overfit_batch_idx:
                break
        data, target = batch
        data = data.reshape(-1, n_features).float()
        assert data.shape[0] != 0
        target = target.flatten().float()
        # Forward pass through the pairwise model
        data = data.to(device)
        y_pred.append(torch.sigmoid(model(data)).cpu().numpy())
        targets.append(target)
    y_pred = np.hstack(y_pred)
    targets = np.hstack(targets)

    if return_pred_only:
        return y_pred

    fpr, tpr, _ = roc_curve(targets, y_pred)
    roc_auc = auc(fpr, tpr)
    pr, rc, f1, _ = precision_recall_fscore_support(targets, y_pred >= thresh_for_f1, beta=1.0, average=mode,
                                                    zero_division=0)

    return roc_auc, np.round(f1, 3)


def train(hyperparams={}, verbose=False, project=None, entity=None, tags=None, group=None,
          save_model=False, load_model_from_wandb_run=None, load_model_from_fpath=None,
          eval_only_split=None, skip_initial_eval=False, pairwise_mode=False):
    init_args = {
        'config': DEFAULT_HYPERPARAMS
    }
    if project is not None:
        init_args.update({'project': project})
    if entity is not None:
        init_args.update({'entity': entity})
    if tags is not None:
        tags = tags.replace(", ", ",").split(",")
        init_args.update({'tags': tags})
    if group is not None:
        init_args.update({'group': group})

    # Start wandb run
    with wandb.init(**init_args) as run:
        wandb.config.update(hyperparams, allow_val_change=True)
        hyp = wandb.config
        # Save hyperparameters as a json file and store in wandb run
        with open(os.path.join(run.dir, 'hyperparameters.json'), 'w') as fh:
            json.dump(dict(hyp), fh)
        wandb.save('hyperparameters.json')

        # Seed everything
        torch.manual_seed(hyp['run_random_seed'])
        random.seed(hyp['run_random_seed'])
        np.random.seed(hyp['run_random_seed'])

        weighted_loss = hyp['weighted_loss']
        batch_size = hyp['batch_size']
        n_epochs = hyp['n_epochs']
        use_lr_scheduler = hyp['use_lr_scheduler']
        hidden_dim = hyp["hidden_dim"]
        n_hidden_layers = hyp["n_hidden_layers"]
        dropout_p = hyp["dropout_p"]
        dropout_only_once = hyp["dropout_only_once"]
        hidden_config = hyp["hidden_config"]
        activation = hyp["activation"]
        add_batchnorm = hyp["batchnorm"]
        neumiss_deq = hyp["neumiss_deq"]
        neumiss_depth = hyp["neumiss_depth"]
        add_neumiss = not hyp['convert_nan']
        negative_slope = hyp["negative_slope"]
        sdp_max_iters = hyp["sdp_max_iters"]
        sdp_eps = hyp["sdp_eps"]
        overfit_batch_idx = hyp['overfit_batch_idx']
        eval_metric_to_idx = {'vmeasure': 0, 'b3_f1': 1} if not pairwise_mode else {'auroc': 0, 'f1': 1}
        dev_opt_metric = hyp['dev_opt_metric'] if hyp['dev_opt_metric'] in eval_metric_to_idx \
            else list(eval_metric_to_idx)[0]

        # Get data loaders (optionally with imputation, normalization)
        train_dataloader, val_dataloader, test_dataloader = get_dataloaders(hyp["dataset"], hyp["dataset_random_seed"],
                                                                            hyp["convert_nan"], hyp["nan_value"],
                                                                            hyp["normalize_data"], hyp["subsample_sz"],
                                                                            hyp["subsample_dev"], pairwise_mode,
                                                                            batch_size)
        n_features = train_dataloader.dataset[0][0].shape[1]

        # Create model with hyperparams
        if not pairwise_mode:
            model = EntResModel(n_features, neumiss_depth, dropout_p, dropout_only_once, add_neumiss,
                                neumiss_deq, hidden_dim, n_hidden_layers, add_batchnorm, activation,
                                negative_slope, hidden_config, sdp_max_iters, sdp_eps)
            # Define loss
            loss_fn = lambda pred, gold: torch.norm(gold - pred)
            # Define eval
            eval_fn = evaluate
        else:
            model = PairwiseModel(n_features, neumiss_depth, dropout_p, dropout_only_once, add_neumiss,
                                  neumiss_deq, hidden_dim, n_hidden_layers, add_batchnorm, activation,
                                  negative_slope, hidden_config)
            # Define loss
            pos_weight = None
            if weighted_loss:
                if overfit_batch_idx > -1:
                    n_pos = \
                        train_dataloader.dataset[overfit_batch_idx * batch_size:(overfit_batch_idx + 1) * batch_size][
                            1].sum()
                    pos_weight = torch.tensor((batch_size - n_pos) / n_pos)
                else:
                    n_pos = train_dataloader.dataset[:][1].sum()
                    pos_weight = torch.tensor((len(train_dataloader.dataset) - n_pos) / n_pos)
            loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)
            # Define eval
            eval_fn = evaluate_pairwise
        logger.info(f"Model loaded: {model}", )

        # Load stored model, if available
        state_dict = None
        if load_model_from_wandb_run is not None:
            state_dict_fpath = wandb.restore('model_state_dict_best.pt',
                                             run_path=load_model_from_wandb_run).name
            state_dict = torch.load(state_dict_fpath, device)
        elif load_model_from_fpath is not None:
            state_dict = torch.load(load_model_from_fpath, device)
        if state_dict is not None:
            model.load_state_dict(state_dict)
            logger.info(f'Loaded stored model.')
        model.to(device)

        if eval_only_split is not None:
            # Run inference and exit
            dataloaders = {
                'train': train_dataloader,
                'dev': val_dataloader,
                'test': test_dataloader
            }
            with torch.no_grad():
                start_time = time.time()
                eval_scores = eval_fn(model, dataloaders[eval_only_split])
                end_time = time.time()
                if verbose:
                    logger.info(
                        f"Eval: {eval_only_split}_{list(eval_metric_to_idx)[0]}={eval_scores[0]}, " +
                        f"{eval_only_split}_{list(eval_metric_to_idx)[1]}={eval_scores[1]}")
                wandb.log({'epoch': 0, f'{eval_only_split}_{list(eval_metric_to_idx)[0]}': eval_scores[0],
                           f'{eval_only_split}_{list(eval_metric_to_idx)[1]}': eval_scores[1]})
        else:
            # Training
            wandb.watch(model)

            optimizer = torch.optim.AdamW(model.parameters(), lr=hyp['lr'])
            if use_lr_scheduler:
                if hyp['lr_scheduler'] == 'plateau':
                    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,
                                                                           mode='min',
                                                                           factor=hyp['lr_factor'],
                                                                           min_lr=hyp['lr_min'],
                                                                           patience=hyp['lr_scheduler_patience'],
                                                                           verbose=True)
                elif hyp['lr_scheduler'] == 'step':
                    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=hyp['lr_step_size'],
                                                                gamma=hyp['lr_gamma'], verbose=True)

            best_dev_state_dict = None
            best_dev_score = -1  # Stores the score of only the specified optimization metric
            best_dev_scores = None  # Contains scores of all metrics
            best_epoch = 0

            if not skip_initial_eval:
                # Get initial model performance on dev (or 'train' for overfitting runs)
                with torch.no_grad():
                    model.eval()
                    if overfit_batch_idx > -1:
                        train_scores = eval_fn(model, train_dataloader, overfit_batch_idx)
                        if verbose:
                            logger.info(f"Initial: train_{list(eval_metric_to_idx)[0]}={train_scores[0]}, " +
                                        f"train_{list(eval_metric_to_idx)[1]}={train_scores[1]}")
                        wandb.log({'epoch': 0, f'train_{list(eval_metric_to_idx)[0]}': train_scores[0],
                                   f'train_{list(eval_metric_to_idx)[1]}': train_scores[1]})
                    else:
                        dev_scores = eval_fn(model, val_dataloader)
                        if verbose:
                            logger.info(f"Initial: dev_{list(eval_metric_to_idx)[0]}={dev_scores[0]}, " +
                                        f"dev_{list(eval_metric_to_idx)[1]}={dev_scores[1]}")
                        wandb.log({'epoch': 0, f'dev_{list(eval_metric_to_idx)[0]}': dev_scores[0],
                                   f'dev_{list(eval_metric_to_idx)[1]}': dev_scores[1]})

            model.train()
            start_time = time.time()  # Tracks full training runtime
            for i in range(n_epochs):
                wandb.log({'epoch': i + 1})
                running_loss = []
                for (idx, batch) in enumerate(tqdm(train_dataloader, desc="Training")):
                    if overfit_batch_idx > -1:
                        if idx < overfit_batch_idx:
                            continue
                        if idx > overfit_batch_idx:
                            break
                    if not pairwise_mode:
                        data, target, _ = batch
                    else:
                        data, target = batch
                    data = data.reshape(-1, n_features).float()
                    if data.shape[0] == 0:
                        # Block contains only one signature
                        continue
                    if add_batchnorm and data.shape[0] == 1:
                        # Block contains only one signature pair; batchnorm throws error
                        continue
                    block_size = get_matrix_size_from_triu(data)
                    target = target.flatten().float()
                    if verbose:
                        logger.info(f"Batch shape: {data.shape}")
                        if not pairwise_mode:
                            logger.info(f"Batch matrix size: {block_size}")

                    # Forward pass through the e2e or pairwise model
                    data, target = data.to(device), target.to(device)
                    output = model(data, block_size, verbose)

                    # Calculate the loss
                    if not pairwise_mode:
                        gold_output = uncompress_target_tensor(target)
                        if verbose:
                            logger.info(f"Gold:\n{gold_output}")
                        loss = loss_fn(output.view_as(gold_output), gold_output) / (2 * block_size)
                    else:
                        if verbose:
                            logger.info(f"Gold:\n{target}")
                        loss = loss_fn(output.view_as(target), target)

                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()

                    if verbose:
                        logger.info(f"Loss = {loss.item()}")
                    running_loss.append(loss.item())
                    wandb.log({'train_loss': np.mean(running_loss)})

                if verbose:
                    logger.info(f"Epoch loss = {np.mean(running_loss)}")

                # Get model performance on dev (or 'train' for overfitting runs)
                with torch.no_grad():
                    model.eval()
                    if overfit_batch_idx > -1:
                        train_scores = eval_fn(model, train_dataloader, overfit_batch_idx)
                        if verbose:
                            logger.info(f"Epoch {i + 1}: train_{list(eval_metric_to_idx)[0]}={train_scores[0]}, " +
                                        f"train_{list(eval_metric_to_idx)[1]}={train_scores[1]}")
                        wandb.log({f'train_{list(eval_metric_to_idx)[0]}': train_scores[0],
                                   f'train_{list(eval_metric_to_idx)[1]}': train_scores[1]})
                        if use_lr_scheduler:
                            if hyp['lr_scheduler'] == 'plateau':
                                scheduler.step(train_scores[eval_metric_to_idx[dev_opt_metric]])
                            elif hyp['lr_scheduler'] == 'step':
                                scheduler.step()
                    else:
                        dev_scores = eval_fn(model, val_dataloader)
                        if verbose:
                            logger.info(f"epoch {i + 1}: dev_{list(eval_metric_to_idx)[0]}={dev_scores[0]}, " +
                                        f"dev_{list(eval_metric_to_idx)[1]}={dev_scores[1]}")
                        wandb.log({f'dev_{list(eval_metric_to_idx)[0]}': dev_scores[0],
                                   f'dev_{list(eval_metric_to_idx)[1]}': dev_scores[1]})
                        dev_opt_score = dev_scores[eval_metric_to_idx[dev_opt_metric]]
                        if dev_opt_score > best_dev_score:
                            if verbose:
                                logger.info(f"New best dev {dev_opt_metric} score @ epoch{i+1}: {dev_opt_score}")
                            best_epoch = i
                            best_dev_score = dev_opt_score
                            best_dev_scores = dev_scores
                            best_dev_state_dict = copy.deepcopy(model.state_dict())
                        if use_lr_scheduler:
                            if hyp['lr_scheduler'] == 'plateau':
                                scheduler.step(dev_scores[eval_metric_to_idx[dev_opt_metric]])
                            elif hyp['lr_scheduler'] == 'step':
                                scheduler.step()
                model.train()

            end_time = time.time()

            if overfit_batch_idx == -1:
                # Evaluate best dev model on test
                model.load_state_dict(best_dev_state_dict)
                with torch.no_grad():
                    model.eval()
                    test_scores = eval_fn(model, test_dataloader)
                    if verbose:
                        logger.info(f"Final: test_{list(eval_metric_to_idx)[0]}={test_scores[0]}, " +
                                    f"test_{list(eval_metric_to_idx)[1]}={test_scores[1]}")
                    # Log final metrics
                    wandb.log({'best_dev_epoch': best_epoch + 1,
                               f'best_dev_{list(eval_metric_to_idx)[0]}': best_dev_scores[0],
                               f'best_dev_{list(eval_metric_to_idx)[1]}': best_dev_scores[1],
                               f'best_test_{list(eval_metric_to_idx)[0]}': test_scores[0],
                               f'best_test_{list(eval_metric_to_idx)[1]}': test_scores[1]})

        run.summary["z_model_parameters"] = count_parameters(model)
        run.summary["z_run_time"] = round(end_time - start_time)
        run.summary["z_run_dir_path"] = run.dir

        # Save models
        if save_model:
            torch.save(best_dev_state_dict, os.path.join(run.dir, 'model_state_dict_best.pt'))
            wandb.save('model_state_dict_best.pt')

        logger.info(f"Run directory: {run.dir}")
        logger.info("End of train() call")


if __name__ == '__main__':
    # Read cmd line args
    parser = Parser(add_training_args=True)
    # Handle additional arbitrary arguments
    _, unknown = parser.parse_known_args()
    make_false_args = []
    for arg in unknown:
        if arg.startswith("--"):
            arg_split = arg.split('=')
            argument_name = arg_split[0]
            if argument_name[2:] in DEFAULT_HYPERPARAMS:
                argument_type = type(DEFAULT_HYPERPARAMS[argument_name[2:]])
                if argument_type == bool:
                    if len(arg_split) > 1 and arg_split[1].lower() == 'false':
                        parser.add_argument(argument_name, type=str)
                        make_false_args.append(argument_name[2:])
                    else:
                        parser.add_argument(argument_name, action='store_true')
                else:
                    parser.add_argument(argument_name, type=argument_type)
    args = parser.parse_args().__dict__
    for false_arg in make_false_args:
        args[false_arg] = False
    hyp_args = {k: v for k, v in args.items() if k in DEFAULT_HYPERPARAMS}
    logger.info("Script arguments:")
    logger.info(args)

    if args['cpu']:
        device = torch.device("cpu")
    else:
        device = torch.device(
            "cuda" if torch.cuda.is_available() else "cpu"
        )
    logger.info(f"Using device={device}")

    wandb.login()
    if args['wandb_sweep_params'] is not None:
        logger.info("Sweep mode")
        with open(args['wandb_sweep_params'], 'r') as fh:
            sweep_params = json.load(fh)

        sweep_config = {
            'method': args['wandb_sweep_method'],
            'name': args['wandb_sweep_name'],
            'metric': {
                'name': args['wandb_sweep_metric_name'],
                'goal': args['wandb_sweep_metric_goal'],
            },
            'parameters': sweep_params,
        }
        if not args['wandb_no_early_terminate']:
            sweep_config.update({
                'early_terminate': {
                    'type': 'hyperband',
                    'min_iter': 5
                }
            })

        # Init sweep
        sweep_id = args["wandb_sweep_id"]
        if sweep_id is None:
            sweep_id = wandb.sweep(sweep=sweep_config,
                                   project=args['wandb_project'],
                                   entity=args['wandb_entity'])

        # Start sweep job
        wandb.agent(sweep_id,
                    function=lambda: train(hyperparams=hyp_args),
                    count=args['wandb_max_runs'])

        logger.info("End of sweep")
    else:
        logger.info("Single-run mode")
        try:
            with open(args['wandb_run_params'], 'r') as fh:
                run_params = json.load(fh)
        except:
            logger.info("Run config could not be loaded; using defaults.")
            run_params = {}
        run_params.update(hyp_args)
        train(hyperparams=run_params,
              verbose=not args['silent'],
              project=args['wandb_project'],
              entity=args['wandb_entity'],
              tags=args['wandb_tags'],
              group=args['wandb_group'],
              save_model=args['save_model'],
              load_model_from_wandb_run=args['load_model_from_wandb_run'],
              load_model_from_fpath=args['load_model_from_fpath'],
              eval_only_split=args['eval_only_split'],
              skip_initial_eval=args['skip_initial_eval'],
              pairwise_mode=args['pairwise_mode'])
        logger.info("End of run")
